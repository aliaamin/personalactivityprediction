---
title: "Personal Activity Prediction"
author: "A. Amin"
date: "Saturday, June 06, 2015"
output: html_document #ioslides_presentation
---

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

## Analysis

#### Step 1. Load library

```{r warning=FALSE,  message=FALSE}
library(AppliedPredictiveModeling)
library(caret)
library(ggplot2)
library(randomForest)
```

#### Step 2: Data preparation
Load csv and identify NA values

```{r warning=FALSE,  message=FALSE}

data = read.csv("pml-training.csv", na.strings=c("NA","","#DIV/0!"), sep=",")
val_data = read.csv("pml-testing.csv", na.strings=c("NA","","#DIV/0!"), sep=",")
```


Remove the first 7 columns from the training, testing, and validating dataset because usernames and timestamps should not be predictors of the activities.

```{r warning=FALSE,  message=FALSE}

data = data[,-seq(1:7)]
val_data = val_data[,-seq(1:7)]

na_values <- sapply(data, function (x) any(is.na(x) | x == ""))
usable_columns_names <- names(na_values)[!na_values]
```

After dataset clean up, it is found there are 52 predictors columns and the result ("classe" column). 
```{r warning=FALSE,  message=FALSE}
print(usable_columns_names)
```
#### Step 3: Determine the training, testing and validating dataset.

We can finally define the training dataset, testing dataset, and the validating dataset.
```{r warning=FALSE,  message=FALSE}
data <- data[,usable_columns_names]
inTrain <- createDataPartition(y=data$classe, p=0.60, list=FALSE)
training <- data[inTrain,]
testing <- data[-inTrain,]

validating <- val_data[,subset(usable_columns_names, usable_columns_names!="classe")]

x <- rbind(dim(training),dim(testing),dim(validating))
rownames(x) <- c("training dataset","testing dataset","validating dataset")
colnames(x) <- c("#row","#column")
print(x)
```

#### Step 4: Quick evaluation of some predictors
We want to view what is the density of each predictors to each other,  the bigger the difference between predictors, the higher chance the prediction will be able to provide a correct prediction. Due to the high number of predictors, we will only show a few first and last selected predictors as examples.

```{r warning=FALSE,  message=FALSE}
featurePlot(x=training[,c(1:3,50:52)], y=training[,53], plot="density", scales=list(x=list(relation="free"), y=list(relation="free")), layout = c(3, 2))

```

#### Step 5: Experiment with different methods and identify the method with the highest accuracy.

Using the training dataset, predict the model using the random forest method.
```{r warning=FALSE,  message=FALSE}
# Random forest method
system.time(model1 <- train(classe ~ ., data=training, method="rf"))
```

With this model make prediction using the testing dataset and check accuracy of prediction, including the estimated out of sample error, with one round of cross-validation.
```{r warning=FALSE,  message=FALSE}
predictions1 <- predict(model1, testing)
print(a <- confusionMatrix(predictions1, testing$classe))
```

Moreover, try several other machine learning method to see if we are able to increase the accuracy of our prediction, including the estimated out of sample error, as seen in each one round of cross-validation.

```{r warning=FALSE,  message=FALSE}
# Naive Bayes method
train_control <- trainControl(method="cv", number=10)
system.time(model2 <- train(classe~., data=training, trControl=train_control, method="nb"))
predictions2 <- predict(model2, testing)
print(b <- confusionMatrix(predictions2, testing$classe))

```

```{r warning=FALSE,  message=FALSE}
# CART method
system.time(model3 <- train(classe~., data=training, method="rpart"))
predictions3 <- predict(model3, testing)
print(c <- confusionMatrix(predictions3, testing$classe))

```

Finally, herewith the summary of the different methods investigated.
```{r warning=FALSE,  message=FALSE}
x <- matrix(c(a$overall['Accuracy'],b$overall['Accuracy'],c$overall['Accuracy'],a$overall['Kappa'],b$overall['Kappa'],c$overall['Kappa']),ncol=2,nrow=3,byrow=FALSE)

rownames(x) <- c("Random Forest method","Naive Bayes method", "CART method")
colnames(x) <- c("Accuracy","Kappa")
print(x)
```

From the table above we can conclude that the best performer is the Random Forest method. Thus we will use that model to make prediction for the validating dataset.

#### Step 6: Use the best method to predict on the validating dataset

```{r warning=FALSE,  message=FALSE}
predictions4 <- predict(model1, validating)
answer <- cbind(val_data, predictions4)
answer <- answer[c("problem_id","predictions4")]
print(answer)
```

#### Step 7: Write prediction results to files

Write the answer to the file as requested in the assignment.
```{r warning=FALSE,  message=FALSE}
pml_write_files = function(x){
        n = length(x)
        path <- "answers"
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=file.path(path, filename),quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}
pml_write_files(predictions4)

```
#### Step 8: Done. 
